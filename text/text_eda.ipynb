{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:33.712178Z",
     "start_time": "2026-01-18T18:58:33.598232Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.executable"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/silvia/University/Dyploma/Analysis_of_biased_news/.venv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:36.823372Z",
     "start_time": "2026-01-18T18:58:33.720079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ],
   "id": "d53ac51ea60cdd9c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:36.966231Z",
     "start_time": "2026-01-18T18:58:36.868748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pl.read_excel(\"../data/labeled_dataset.xlsx\")\n",
    "data = data.rename({\"__UNNAMED__0\": \"id\"})"
   ],
   "id": "671b4d565e16edb7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic Analysis",
   "id": "860b66ec2cf41144"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:37.029616Z",
     "start_time": "2026-01-18T18:58:36.966730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_documents = len(data)\n",
    "print(f\"Total number of documents: {num_documents:,}\")\n",
    "\n",
    "data = data.with_columns(\n",
    "    pl.col(\"sentence\").str.len_chars().alias(\"char_length\"),\n",
    "    pl.col(\"sentence\").str.split(\" \").list.len().alias(\"word_count\")\n",
    ")\n",
    "\n",
    "print(f\"\\nDocument Length Statistics (characters):\")\n",
    "print(f\"  Mean: {data['char_length'].mean():.2f}\")\n",
    "print(f\"  Median: {data['char_length'].median():.2f}\")\n",
    "print(f\"  Min: {data['char_length'].min()}\")\n",
    "print(f\"  Max: {data['char_length'].max()}\")\n",
    "print(f\"  Std Dev: {data['char_length'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nDocument Length Statistics (words):\")\n",
    "print(f\"  Mean: {data['word_count'].mean():.2f}\")\n",
    "print(f\"  Median: {data['word_count'].median():.2f}\")\n",
    "print(f\"  Min: {data['word_count'].min()}\")\n",
    "print(f\"  Max: {data['word_count'].max()}\")\n",
    "print(f\"  Std Dev: {data['word_count'].std():.2f}\")\n",
    "\n",
    "all_text = \" \".join(data[\"sentence\"].to_list())\n",
    "words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
    "vocabulary = set(words)\n",
    "print(f\"\\nVocabulary size (unique words): {len(vocabulary):,}\")\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "print(f\"Average word length: {np.mean([len(w) for w in words]):.2f} characters\")"
   ],
   "id": "56365604bf6aeff9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 1,700\n",
      "\n",
      "Document Length Statistics (characters):\n",
      "  Mean: 209.72\n",
      "  Median: 206.00\n",
      "  Min: 42\n",
      "  Max: 606\n",
      "  Std Dev: 72.42\n",
      "\n",
      "Document Length Statistics (words):\n",
      "  Mean: 33.48\n",
      "  Median: 33.00\n",
      "  Min: 7\n",
      "  Max: 100\n",
      "  Std Dev: 11.93\n",
      "\n",
      "Vocabulary size (unique words): 8,864\n",
      "Total words: 58,534\n",
      "Average word length: 5.01 characters\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution of word frequencies",
   "id": "3d6614527265c9c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:37.065999Z",
     "start_time": "2026-01-18T18:58:37.037375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Unigram\n",
    "word_freq = Counter(words)\n",
    "print(f\"TOp 20 most common words (unigram):\")\n",
    "for word, freq in word_freq.most_common(20):\n",
    "    print(f\"  {word:20s}: {freq:6,} ({freq/len(words)*100:.2f}%)\")\n",
    "\n",
    "#Bigram\n",
    "bigrams = [\" \".join(words[i:i+2]) for i in range(len(words)-1)]\n",
    "bigram_freq = Counter(bigrams)\n",
    "print(f\"\\nTop 20 most common bigrams:\")\n",
    "for bigram, freq in bigram_freq.most_common(20):\n",
    "    print(f\"  {bigram:35s}: {freq:6,}\")"
   ],
   "id": "7dd3d28f35e82d26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOp 20 most common words (unigram):\n",
      "  the                 :  3,183 (5.44%)\n",
      "  to                  :  1,753 (2.99%)\n",
      "  of                  :  1,618 (2.76%)\n",
      "  and                 :  1,422 (2.43%)\n",
      "  a                   :  1,340 (2.29%)\n",
      "  in                  :  1,106 (1.89%)\n",
      "  s                   :    813 (1.39%)\n",
      "  that                :    807 (1.38%)\n",
      "  for                 :    614 (1.05%)\n",
      "  on                  :    552 (0.94%)\n",
      "  trump               :    479 (0.82%)\n",
      "  is                  :    458 (0.78%)\n",
      "  as                  :    405 (0.69%)\n",
      "  by                  :    342 (0.58%)\n",
      "  with                :    326 (0.56%)\n",
      "  has                 :    316 (0.54%)\n",
      "  are                 :    300 (0.51%)\n",
      "  have                :    299 (0.51%)\n",
      "  it                  :    295 (0.50%)\n",
      "  his                 :    281 (0.48%)\n",
      "\n",
      "Top 20 most common bigrams:\n",
      "  of the                             :    302\n",
      "  in the                             :    270\n",
      "  u s                                :    152\n",
      "  to the                             :    148\n",
      "  donald trump                       :    138\n",
      "  trump s                            :    112\n",
      "  on the                             :    104\n",
      "  for the                            :    101\n",
      "  president donald                   :     94\n",
      "  and the                            :     87\n",
      "  that the                           :     85\n",
      "  in a                               :     75\n",
      "  of a                               :     74\n",
      "  the u                              :     70\n",
      "  at the                             :     68\n",
      "  to be                              :     66\n",
      "  as a                               :     63\n",
      "  from the                           :     60\n",
      "  the coronavirus                    :     59\n",
      "  it s                               :     58\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization and preprocessing evaluation",
   "id": "af7d3a1276097502"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:38.685346Z",
     "start_time": "2026-01-18T18:58:37.068303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalnum() and t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "sample_text = data[\"sentence\"][0]\n",
    "print(f\"\\nOriginal text sample:\")\n",
    "print(f\"  {sample_text[:200]}...\")\n",
    "print(f\"\\nPreprocessed tokens:\")\n",
    "preprocessed = preprocess_text(sample_text)\n",
    "print(f\"  {preprocessed[:30]}\")\n",
    "\n",
    "stopword_count = sum(1 for w in words if w.lower() in stop_words)\n",
    "print(f\"\\nStopwords analysis:\")\n",
    "print(f\"  Stopwords found: {stopword_count:,} ({stopword_count/len(words)*100:.2f}%)\")\n",
    "print(f\"  Words after stopword removal: {len(words) - stopword_count:,}\")\n",
    "\n",
    "stopwords_in_corpus = [w for w in words if w.lower() in stop_words]\n",
    "stopword_freq = Counter(stopwords_in_corpus)\n",
    "print(f\"\\n  Top 10 most frequent stopwords:\")\n",
    "for word, freq in stopword_freq.most_common(10):\n",
    "    print(f\"    {word:15s}: {freq:6,}\")"
   ],
   "id": "6529243bed4e141d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text sample:\n",
      "  YouTube is making clear there will be no “birtherism” on its platform during this year’s U.S. presidential election – a belated response to a type of conspiracy theory more prevalent in the 2012 race....\n",
      "\n",
      "Preprocessed tokens:\n",
      "  ['youtube', 'making', 'clear', 'birtherism', 'platform', 'year', 'presidential', 'election', 'belated', 'response', 'type', 'conspiracy', 'theory', 'prevalent', '2012', 'race']\n",
      "\n",
      "Stopwords analysis:\n",
      "  Stopwords found: 23,759 (40.59%)\n",
      "  Words after stopword removal: 34,775\n",
      "\n",
      "  Top 10 most frequent stopwords:\n",
      "    the            :  3,183\n",
      "    to             :  1,753\n",
      "    of             :  1,618\n",
      "    and            :  1,422\n",
      "    a              :  1,340\n",
      "    in             :  1,106\n",
      "    s              :    813\n",
      "    that           :    807\n",
      "    for            :    614\n",
      "    on             :    552\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Exploration",
   "id": "ef01161c67f4c029"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:38.888514Z",
     "start_time": "2026-01-18T18:58:38.725489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_sentiment(text):\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "    except:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "print(\"\\nCalculating sentiment scores (sample of 500 documents)...\")\n",
    "sample_size = min(500, len(data))\n",
    "sentiments = [get_sentiment(text) for text in data[\"sentence\"][:sample_size].to_list()]\n",
    "polarities = [s[0] for s in sentiments]\n",
    "subjectivities = [s[1] for s in sentiments]\n",
    "\n",
    "print(f\"\\nSentiment Statistics:\")\n",
    "print(f\"  Polarity (range: -1 to 1, negative to positive):\")\n",
    "print(f\"    Mean: {np.mean(polarities):.3f}\")\n",
    "print(f\"    Median: {np.median(polarities):.3f}\")\n",
    "print(f\"    Std Dev: {np.std(polarities):.3f}\")\n",
    "\n",
    "print(f\"\\n  Subjectivity (range: 0 to 1, objective to subjective):\")\n",
    "print(f\"    Mean: {np.mean(subjectivities):.3f}\")\n",
    "print(f\"    Median: {np.median(subjectivities):.3f}\")\n",
    "print(f\"    Std Dev: {np.std(subjectivities):.3f}\")\n",
    "\n",
    "# Sentiment distribution\n",
    "positive = sum(1 for p in polarities if p > 0.1)\n",
    "negative = sum(1 for p in polarities if p < -0.1)\n",
    "neutral = sample_size - positive - negative\n",
    "\n",
    "print(f\"\\n  Sentiment Distribution:\")\n",
    "print(f\"    Positive: {positive:4} ({positive/sample_size*100:.1f}%)\")\n",
    "print(f\"    Neutral:  {neutral:4} ({neutral/sample_size*100:.1f}%)\")\n",
    "print(f\"    Negative: {negative:4} ({negative/sample_size*100:.1f}%)\")"
   ],
   "id": "16137b0c2618b975",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating sentiment scores (sample of 500 documents)...\n",
      "\n",
      "Sentiment Statistics:\n",
      "  Polarity (range: -1 to 1, negative to positive):\n",
      "    Mean: 0.029\n",
      "    Median: 0.000\n",
      "    Std Dev: 0.202\n",
      "\n",
      "  Subjectivity (range: 0 to 1, objective to subjective):\n",
      "    Mean: 0.384\n",
      "    Median: 0.399\n",
      "    Std Dev: 0.244\n",
      "\n",
      "  Sentiment Distribution:\n",
      "    Positive:  145 (29.0%)\n",
      "    Neutral:   269 (53.8%)\n",
      "    Negative:   86 (17.2%)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Topic Exploration",
   "id": "a9c57c17a68f3824"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:39.926136Z",
     "start_time": "2026-01-18T18:58:38.889369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TF-IDF Analysis\n",
    "print(\"\\nPerforming TF-IDF analysis...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "sample_texts = data[\"sentence\"][:1000].to_list()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_texts)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_indices = tfidf_scores.argsort()[-20:][::-1]\n",
    "\n",
    "print(f\"\\nTop 20 keywords by TF-IDF score:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"  {feature_names[idx]:30s}: {tfidf_scores[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\\nPerforming LDA topic modeling (5 topics)...\")\n",
    "n_topics = 5\n",
    "n_top_words = 10\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(sample_texts)\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=10\n",
    ")\n",
    "lda.fit(count_matrix)\n",
    "\n",
    "feature_names_lda = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nTop {n_top_words} words for each topic:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names_lda[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")"
   ],
   "id": "8e4a25ab9f33171d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing TF-IDF analysis...\n",
      "\n",
      "Top 20 keywords by TF-IDF score:\n",
      "  trump                         : 0.0655\n",
      "  women                         : 0.0410\n",
      "  president                     : 0.0391\n",
      "  new                           : 0.0381\n",
      "  abortion                      : 0.0352\n",
      "  people                        : 0.0347\n",
      "  climate                       : 0.0341\n",
      "  said                          : 0.0313\n",
      "  anti                          : 0.0299\n",
      "  coronavirus                   : 0.0297\n",
      "  year                          : 0.0266\n",
      "  student                       : 0.0262\n",
      "  democrats                     : 0.0256\n",
      "  american                      : 0.0243\n",
      "  donald                        : 0.0238\n",
      "  donald trump                  : 0.0238\n",
      "  world                         : 0.0225\n",
      "  media                         : 0.0223\n",
      "  years                         : 0.0221\n",
      "  change                        : 0.0219\n",
      "\n",
      "\n",
      "Performing LDA topic modeling (5 topics)...\n",
      "\n",
      "Top 10 words for each topic:\n",
      "\n",
      "Topic 1: abortion, women, climate, state, abortions, health, new, care, place, right\n",
      "\n",
      "Topic 2: new, abortion, people, green, immigration, deal, years, like, pro, make\n",
      "\n",
      "Topic 3: trump, student, debt, administration, president, border, loan, donald, said, migrants\n",
      "\n",
      "Topic 4: women, anti, vaccine, coronavirus, college, world, vaccines, said, soccer, children\n",
      "\n",
      "Topic 5: trump, president, climate, donald, change, biden, house, white, said, people\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Noise Identification",
   "id": "60b9521726a9d7b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:40.062778Z",
     "start_time": "2026-01-18T18:58:39.984056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "null_count = data[\"sentence\"].null_count()\n",
    "print(f\"\\nNull/Empty documents: {null_count}\")\n",
    "\n",
    "# Check for very short documents (potential spam)\n",
    "very_short = data.filter(pl.col(\"word_count\") < 5).height\n",
    "print(f\"Very short documents (<5 words): {very_short} ({very_short/num_documents*100:.2f}%)\")\n",
    "\n",
    "# Check for very long documents (potential concatenation issues)\n",
    "very_long = data.filter(pl.col(\"word_count\") > 200).height\n",
    "print(f\"Very long documents (>200 words): {very_long} ({very_long/num_documents*100:.2f}%)\")\n",
    "\n",
    "# Check for encoding issues\n",
    "encoding_issues = 0\n",
    "problematic_chars = ['â€™', 'â€\"', 'â€¦', 'â€˜', 'â€œ', '�']\n",
    "for text in data[\"sentence\"][:1000].to_list():\n",
    "    if any(char in text for char in problematic_chars):\n",
    "        encoding_issues += 1\n",
    "\n",
    "print(f\"\\nDocuments with encoding issues (sample): {encoding_issues} ({encoding_issues/min(1000, num_documents)*100:.2f}%)\")\n",
    "print(f\"  Common problematic patterns: {', '.join(problematic_chars)}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.group_by(\"sentence\").agg(pl.len().alias(\"count\")).filter(pl.col(\"count\") > 1)\n",
    "print(f\"\\nDuplicate sentences: {duplicates.height}\")\n",
    "\n",
    "# Check for non-English text (simple heuristic)\n",
    "def contains_non_ascii(text):\n",
    "    return not all(ord(c) < 128 for c in text)\n",
    "\n",
    "non_ascii_count = sum(1 for text in data[\"sentence\"][:1000].to_list() if contains_non_ascii(text))\n",
    "print(f\"\\nDocuments with non-ASCII characters (sample): {non_ascii_count} ({non_ascii_count/min(1000, num_documents)*100:.2f}%)\")\n",
    "\n",
    "# Check for URL presence\n",
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "urls_count = sum(1 for text in data[\"sentence\"].to_list() if re.search(url_pattern, text))\n",
    "print(f\"Documents containing URLs: {urls_count} ({urls_count/num_documents*100:.2f}%)\")\n",
    "\n",
    "# Special characters ratio\n",
    "def special_char_ratio(text):\n",
    "    special = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "    return special / len(text) if len(text) > 0 else 0\n",
    "\n",
    "special_ratios = [special_char_ratio(text) for text in data[\"sentence\"][:1000].to_list()]\n",
    "high_special = sum(1 for r in special_ratios if r > 0.2)\n",
    "print(f\"\\nDocuments with high special character ratio (>20%): {high_special} ({high_special/min(1000, num_documents)*100:.2f}%)\")"
   ],
   "id": "59b368c1654db02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null/Empty documents: 0\n",
      "Very short documents (<5 words): 0 (0.00%)\n",
      "Very long documents (>200 words): 0 (0.00%)\n",
      "\n",
      "Documents with encoding issues (sample): 0 (0.00%)\n",
      "  Common problematic patterns: â€™, â€\", â€¦, â€˜, â€œ, �\n",
      "\n",
      "Duplicate sentences: 0\n",
      "\n",
      "Documents with non-ASCII characters (sample): 435 (43.50%)\n",
      "Documents containing URLs: 0 (0.00%)\n",
      "\n",
      "Documents with high special character ratio (>20%): 0 (0.00%)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:58:40.154250Z",
     "start_time": "2026-01-18T18:58:40.063690Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b707e1e9708d6ed0",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark .venv)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
